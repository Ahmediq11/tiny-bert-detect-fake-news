{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 1. Environment Setup\n",
        "\n",
        "This first cell prepares our environment. It installs the necessary libraries for data handling, modeling, and visualization.\n",
        "\n",
        "- **`transformers`, `accelerate`, `datasets`**: The core Hugging Face stack for loading models, speeding up training, and handling data.\n",
        "- **`openpyxl`**: A required library for `pandas` to be able to read and write Excel files (`.xlsx`).\n",
        "- **`seaborn`, `matplotlib`**: Standard Python libraries for creating visualizations.\n",
        "- **`warnings.filterwarnings('ignore')`**: This line is included to suppress warnings during execution, keeping the output clean for this demonstration. This is generally not recommended for production code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U transformers\n",
        "# !pip install -U accelerate\n",
        "# !pip install -U datasets\n",
        "# !pip install -U bertviz\n",
        "# !pip install -U umap-learn\n",
        "# !pip install seaborn --upgrade\n",
        "\n",
        "# !pip install -U openpyxl\n",
        "\n",
        "# Don't do in production. Doing now to keep output clean for understanding\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 2. Data Loading and Cleaning\n",
        "\n",
        "We load our dataset, which is an Excel file containing news articles, using `pandas`. A crucial first step in any machine learning project is to handle missing data. We check for null values using `.isnull().sum()` and then remove any rows with missing values using `.dropna()` to ensure our dataset is clean and ready for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/master/fake_news.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()\n",
        "df = df.dropna()\n",
        "\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "With a clean dataset, we explore its characteristics.\n",
        "\n",
        "1.  **Class Distribution**: We check the balance between 'Real' and 'Fake' news articles using `.value_counts()` and visualize it with a bar chart. Our dataset appears to be well-balanced.\n",
        "2.  **Token Length Analysis**: We estimate the number of tokens in the `title` and `text` of each article (approximating 1.5 tokens per word). Histograms show us the distribution of these lengths. We can see that titles are short, while the article texts have a much wider range of lengths. This notebook will focus on classifying news based on the **title only** for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["df.shape\n", "\n", "df['label'].value_counts()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["df.title"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import matplotlib.pyplot as plt"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_counts = df['label'].value_counts(ascending=True)\n",
        "label_counts.plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5 tokens per word on average\n",
        "df['title_tokens'] = df['title'].apply(lambda x: len(x.split())*1.5)\n",
        "df['text_tokens'] = df['text'].apply(lambda x: len(x.split())*1.5)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "ax[0].hist(df['title_tokens'], bins=50, color = 'skyblue')\n",
        "ax[0].set_title(\"Title Tokens\")\n",
        "\n",
        "ax[1].hist(df['text_tokens'], bins=50, color = 'orange')\n",
        "ax[1].set_title(\"Text Tokens\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è 4. Data Splitting & Formatting\n",
        "\n",
        "To train and evaluate our model properly, we split the data into three distinct sets: a **training set (70%)** to train the model, a **test set (20%)** for final evaluation, and a **validation set (10%)** to monitor performance during training. The `stratify=df['label']` argument ensures that the proportion of 'Real' and 'Fake' news is the same across all three sets.\n",
        "\n",
        "Finally, we convert our pandas DataFrames into a `DatasetDict`, which is the standard data structure used by the Hugging Face `Trainer` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 70% for training, 20% test, 10% validation\n",
        "train, test = train_test_split(df, test_size=0.3, stratify=df['label'])\n",
        "test, validation = train_test_split(test, test_size=1/3, stratify=test['label'])\n",
        "\n",
        "train.shape, test.shape, validation.shape, df.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset = DatasetDict(\n",
        "    {\n",
        "        \"train\": Dataset.from_pandas(train, preserve_index=False),\n",
        "        \"test\": Dataset.from_pandas(test, preserve_index=False),\n",
        "        \"validation\": Dataset.from_pandas(validation, preserve_index=False)\n",
        "    }\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 5. Tokenization\n",
        "\n",
        "Here we prepare the text data for the model. **Tokenization** is the process of converting text into numerical IDs that the model can understand.\n",
        "\n",
        "- **Model Choice**: We choose `distilbert-base-uncased`. This is a smaller, faster, and lighter version of BERT, making it ideal for quick training and inference without a huge sacrifice in performance.\n",
        "- **`AutoTokenizer`**: We load the specific tokenizer that corresponds to our chosen model to ensure data is processed correctly.\n",
        "- **Applying Tokenization**: We create a `tokenize` function that takes a batch of data and applies the tokenizer to the `title` column. Using `.map()`, we efficiently apply this function to our entire dataset. `padding=True` and `truncation=True` handle variable-length titles by making them all the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "text = \"Machine learning is awesome!! Thanks KGP Talkie.\"\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "distilbert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "distilbert_tokens = distilbert_tokenizer.tokenize(text)\n",
        "\n",
        "# model_ckpt = \"google/mobilebert-uncased\"\n",
        "# mobilebert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "# mobilebert_tokens = mobilebert_tokenizer.tokenize(text)\n",
        "\n",
        "# model_ckpt = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "# tinybert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "# tinybert_tokens = tinybert_tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert_tokenizer,\n",
        "# mobilebert_tokenizer,\n",
        "# tinybert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    temp = distilbert_tokenizer(batch['title'], padding=True, truncation=True)\n",
        "    return temp\n",
        "\n",
        "print(tokenize(dataset['train'][:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(tokenize, batch_size=None, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 6. Model Configuration\n",
        "\n",
        "Now we load and configure the pre-trained DistilBERT model for our specific task.\n",
        "\n",
        "- **Label Mapping**: We create `label2id` and `id2label` dictionaries to map our string labels ('Real', 'Fake') to integer IDs (0, 1). Models work with numbers, so this mapping is essential.\n",
        "- **`AutoConfig`**: We load the model's default configuration and update it with our specific number of labels and our label mappings.\n",
        "- **`AutoModelForSequenceClassification`**: This command downloads the pre-trained DistilBERT model and attaches a new, untrained classification head on top, configured according to our settings. This head is what we will fine-tune.\n",
        "- **Device**: The model is moved to the GPU (`cuda`) if one is available, which significantly speeds up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "import torch\n",
        "\n",
        "label2id = {\"Real\": 0, \"Fake\": 1}\n",
        "id2label = {0:\"Real\", 1:\"Fake\"}\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "# model_ckpt = \"google/mobilebert-uncased\"\n",
        "# model_ckpt = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "\n",
        "\n",
        "num_labels = len(label2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_ckpt, label2id=label2id, id2label=id2label)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 7. Advanced Training Setup\n",
        "\n",
        "We configure the training process using `TrainingArguments` and a custom metrics function. This notebook uses several advanced arguments to improve training stability and efficiency.\n",
        "\n",
        "- **`compute_metrics_evaluate`**: A function that calculates accuracy and a weighted F1-score during evaluation. The F1-score is a good metric that balances precision and recall.\n",
        "- **`TrainingArguments`**: We set key hyperparameters, but also include several improvements:\n",
        "  - **`eval_strategy=\"steps\"`**: Evaluate the model periodically *during* an epoch, not just at the end.\n",
        "  - **`load_best_model_at_end=True`**: The trainer will keep track of the model with the best `accuracy` on the validation set and automatically load it at the end of training.\n",
        "  - **`fp16=True`**: Enables mixed-precision training, which uses a combination of 16-bit and 32-bit floating-point types to speed up training and reduce memory usage on modern GPUs.\n",
        "  - **`warmup_ratio=0.1`**: Implements a learning rate scheduler that starts with a very low learning rate, gradually increases it for the first 10% of training steps, and then decays it. This helps stabilize training in the early stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use sklearn to build compute metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics_evaluate(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# batch_size = 32\n",
        "training_dir = \"train_dir\"\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=training_dir,\n",
        "    num_train_epochs=3,                  # Increased epochs for better convergence on this dataset size\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,      # A common batch size\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # --- Improvements ---\n",
        "    eval_strategy=\"steps\",         # üí° Evaluate during training\n",
        "    eval_steps=100,                      # Evaluate every 100 steps\n",
        "    save_strategy=\"steps\",               # üí° Match saving strategy to evaluation\n",
        "    save_steps=100,\n",
        "    logging_steps=50,                    # Log training loss every 50 steps\n",
        "    load_best_model_at_end=True,         # üöÄ Automatically load the best model\n",
        "    metric_for_best_model=\"accuracy\",    # The metric to monitor for the \"best\" model\n",
        "    save_total_limit=2,                  # Only keep the best model and the most recent one\n",
        "    fp16=True,                           # ‚ö°Ô∏è Enable mixed-precision for faster training (requires a modern GPU)\n",
        "    warmup_ratio=0.1,                    # Use a learning rate scheduler with warmup\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è 8. Model Training\n",
        "\n",
        "We instantiate the `Trainer`, which brings together the model, training arguments, datasets, tokenizer, and metrics function. Calling `trainer.train()` starts the fine-tuning process. The `Trainer` handles all the complexity of the training loop, including moving data to the device, calculating loss, performing backpropagation, updating weights, and running evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics_evaluate,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=distilbert_tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.train()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßê 9. Final Evaluation and Inference\n",
        "\n",
        "After training, we evaluate the final model on the test set, which it has never seen before. \n",
        "\n",
        "1.  **Inference Function**: We create a `get_prediction` function to test the model with a single, custom headline. This shows how the model would be used in a real application.\n",
        "2.  **Test Set Prediction**: We use `trainer.predict()` to get predictions for the entire test set and review the final performance metrics.\n",
        "3.  **Classification Report**: We generate a detailed `classification_report` from `scikit-learn` to see the precision, recall, and F1-score for both the 'Real' and 'Fake' classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Researchers Publish Findings on Efficacy of New Alzheimer's Drug\"\n",
        "\n",
        "def get_prediction(text):\n",
        "    input_encoded = distilbert_tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**input_encoded)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    pred = torch.argmax(logits, dim=1).item()\n",
        "    return id2label[pred]\n",
        "\n",
        "get_prediction(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds_output = trainer.predict(encoded_dataset['test'])\n",
        "preds_output.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
        "y_true = encoded_dataset['test'][:]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, target_names=list(label2id)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ 10. Saving and Using the Model\n",
        "\n",
        "The final step is to save our work and demonstrate an easy way to use the model.\n",
        "\n",
        "- **`trainer.save_model()`**: This command saves the trained model's weights, configuration, and tokenizer information to a directory (named \"fake_news\").\n",
        "- **`pipeline`**: We load the saved model into a `text-classification` pipeline. The pipeline is the easiest way to perform inference, as it wraps all the necessary steps (tokenization, model prediction, and converting the output to a label) into a single, simple function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.save_model(\"fake_news\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('text-classification', model= 'fake_news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["classifier(\"some text data\")"]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
